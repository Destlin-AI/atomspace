#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url} 
\usepackage{slashed}
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "times" "default"
\font_sans "helvet" "default"
\font_typewriter "cmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks true
\pdf_pdfborder true
\pdf_colorlinks true
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 0
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},basewidth={0.45em}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Factoring with Statistical Linguistics
\end_layout

\begin_layout Date
January 2023
\end_layout

\begin_layout Author
Linas Vep≈°tas
\end_layout

\begin_layout Abstract
An attempt is made to develop a mathematical formalism for factoring large
 language graphs into factors that have a symbolic interpretation.
 The OpenCog language learning effort has been attempting to induce grammar,
 syntax and semantics from corpora.
 Most of this work is purely experimental, 
\begin_inset Quotes eld
\end_inset

seat of the pants
\begin_inset Quotes erd
\end_inset

 exploration.
 This document attempts to provide a mathematical notation for that work,
 thus perhaps making it clearer and easier to grasp.
\end_layout

\begin_layout Abstract
The primary focus is on exploring the nature of probability in extremely
 high-dimensional spaces (
\begin_inset Quotes eld
\end_inset

hyperspaces
\begin_inset Quotes erd
\end_inset

), and how traditional linguistic ideas can be applied to factorize probability
 distributions into components.
\end_layout

\begin_layout Section*
Introduction
\end_layout

\begin_layout Standard
A probabilistic description of natural language posits that probability
 theory can be validly applied to word-sequences.
 Given a sequence of words 
\begin_inset Formula $\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 representing a sentence, a paragraph, or a longer text..., one make the
\emph on
 a priori
\emph default
 assummption that it is possible to assign a probability 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 to this sequence.
 It is not philosophically or scientifically obvious that this is a valid
 assumption: the collection of sayable sentences is presumably infinite;
 language changes over time; ever human speaker internalizes slightly different
 grammars and idiomatic expressions.
 Vocabularies are different for technical texts and literary texts.
 While it is true that present-day computers can gather up billions of sentences
 by scraping the web, it can hardly be assumed that these are converging
 to some overall stable probability distribution 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

.
 Thus, assuming that 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 exists is intellectually dangerous.
\end_layout

\begin_layout Standard
None-the-less, we make this assumption, for two reasons.
 First, it is useful.
 Second, if a specific finite-sized corpus is selected and fixed, it is
 a simple and unambiguous matter of counting words and phrases to obtain
 frequency distributions.
 However, even in this case, one should not be naive: the probability space
 of of all sentences in a modest sized corpus is immense.
 It would be nice if one could work with smallar factors.
 Linguists have already exposed what these factors could be: nouns, verbs,
 grammatical relationships.
 One goal of this text is to formalize the relationship between grammar
 and proability spaces, using mathematical notation rather than hot air.
 It is hoped this will make things clearer.
 Another goal is to extend this analysis into the domain of semantics and
 
\begin_inset Quotes eld
\end_inset

common sense
\begin_inset Quotes erd
\end_inset

.
 This second goal won't be met, other than to suggest that exactly the same
 methods that allowed low-level syntactic factorization to be performed,
 can also be applied, again, at more abstract levels.
 A third goal is to use this mathematical machinery to guide the development
 of software for performing this analysis, to guide future experiments,
 and to provde a better theoretical foundation for what has so far been
 a seat-of-the-pants effort.
\end_layout

\begin_layout Subsection*
Parsing
\end_layout

\begin_layout Standard
The seat-of-the-pants effort so far has been forcused on the automated extractio
n of a grammar from a text corpus.
 Issues of text segmentation are completely avoided: it is assumed that
 the corpus consists of words, unabiguously separated by blank spaces.
 These are avoided because segmentation is a deep, difficult and interesting
 problem, and tackling it takes us afield.
 Likewise, issues of morphology are also ignored.
 Both of these are fundamentally important.
 It is hoped (beleived by the author) that the techniques described here
 will also be applicable there.
 But for now, its easiest to presume that there is a text corpus, consisting
 of well-defined 
\begin_inset Quotes eld
\end_inset

words
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
One statistical approach to parsing is to simply count word-pairs in the
 sample corpus, and then to compute the pairwise point mutual information
 
\begin_inset Formula $MI\left(u,w\right)$
\end_inset

 for all pairs.
 This mutual information can be used to create a Maximum Spanning Tree (MST)
 parse: to consider all possible trees spanning all words in a sentence
 (or block of text) and then select the one that maximizes the grand total
 MI, summed over the word-pairs in the tree.
 It is also useful to consider the Maximal Planar Graph (MPG) parse: starting
 with the MST tree, add edges to create cycles (loops), while still maximizing
 the total MI.
 That such MST parses correspond to reasonable linguistic structure has
 been widely explored over several decades.
\end_layout

\begin_layout Standard
A grammar can be extracted by taking such MST/MPG parses and cutting each
 edge in half, and retaining, as a 
\begin_inset Quotes eld
\end_inset

connector label
\begin_inset Quotes erd
\end_inset

, what word that edge used to connect to.
 The result of such chopping-up are the so-called 
\begin_inset Quotes eld
\end_inset

disjuncts
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

jigsaw puzzle pieces
\begin_inset Quotes erd
\end_inset

 of Link Grammar.
 These can be reassmbled again, to obtain syntactic parses of sentences.
 Link Grammar works: there are extensive hand-curated dictionaries for English,
 Russian and Thai, with smaller dictionaries for another dozen natural languages.
 The English dictionary might be the most accurate/sophisticated parsing
 system currently available.
 
\end_layout

\begin_layout Standard
Link Grammar grammars can be converted to other formalisms; 
\emph on
e.g.

\emph default
 Head-Phrase Sturcture Grammar (HPSG) and so on.
 It can be shown that Link Grammar is 
\begin_inset Quotes eld
\end_inset

isomorphic
\begin_inset Quotes erd
\end_inset

 to Combinatory Categorial Grammars (CCG).
 The quotes around 
\begin_inset Quotes eld
\end_inset

isomorphic
\begin_inset Quotes erd
\end_inset

 have less to do about the math, than what a typical linguist might find
 acceptable in the mapping.
 For the remainder of this text, we assume that any grammar formalism is
 acceptable, and that they are all inter-convertible, interchangeable with
 one another, at least weakly, if not strongly.
 The goal of this text is to expose the relationship between statistics
 and grammar, rather than to quibble the finer points of linguistics.
 When the text below says things like 
\begin_inset Quotes eld
\end_inset

a relationship 
\begin_inset Formula $r\left(w_{1},w_{2},w_{3}\right)$
\end_inset

 between three words 
\begin_inset Formula $\left(w_{1},w_{2},w_{3}\right)$
\end_inset


\begin_inset Quotes erd
\end_inset

 you are free to imagine any grammar formlism that you wish, involving subjects,
 verbs and objects and so on.
 However, Link Grammar will remain the touchstone, as it is the most compatible
 with probability theory.
 Thus, a general acquiantance with Link Grammar is strongly recommended.
\end_layout

\begin_layout Section*
Factorization
\end_layout

\begin_layout Standard
The notion of factorization is to take some large blob, and pick it apart
 into components: to factor a matrix into block-diagonal components, to
 factor an integer into primes.
 Probability distributions over statistically independent variables factorize
 trivially: this is what is meant by the words 
\begin_inset Quotes eld
\end_inset

statistically indepenent
\begin_inset Quotes erd
\end_inset

.
 Probability distributions over language are not, of course, statistically
 independent, and thus are not strictly factorizable.
 None-the-less, they are almost so; the goal is to identify the strongly
 connected components and separate them from one-another, identifying the
 weaker connections.
\end_layout

\begin_layout Standard
Lets try to capture this idea using mathematical notation.
 To recap the story so far: Let 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 be the probability of observing 
\begin_inset Formula $n$
\end_inset

 words in a sentence (or block of text).
 The space of sequences 
\begin_inset Formula $\left\{ \left(w_{1},w_{2},\cdots,w_{n}\right)\right\} $
\end_inset

 is a Cartesian product space, and 
\begin_inset Formula $p$
\end_inset

 is a measure upon that space.
 
\end_layout

\begin_layout Standard
The goal of factorization is to approximate this measure by factorizing
 it into parts, where the parts are given by parsing via conventional linguistic
 theory.
 That is, we presume that relations 
\begin_inset Formula $r_{i}$
\end_inset

 between small sets of words can be found, such that the following holds,
 approximately:
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx p\left(r_{1}\left\{ w\right\} \right)p\left(r_{2}\left\{ w\right\} \right)\cdots p\left(r_{k}\left\{ w\right\} \right)
\]

\end_inset

where 
\begin_inset Formula $r_{1},r_{2},\cdots,r_{k}$
\end_inset

 are syntactic relations (subject, verb, object...) and the 
\begin_inset Formula $\left\{ w\right\} $
\end_inset

 are the set of words taking part in that particular relationship.
 For example, the relation might be a subject-verb-object relationship;
 the set 
\begin_inset Formula $\left\{ w\right\} $
\end_inset

 then consists of only three words.
 The point here is that the 
\begin_inset Formula $r_{i}$
\end_inset

 are 
\begin_inset Quotes eld
\end_inset

small
\begin_inset Quotes erd
\end_inset

, whereas 
\begin_inset Formula $\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

large
\begin_inset Quotes erd
\end_inset

.
 The goal is to grapple wth complexity be finding sutiable recurring patterns.
 Lingusits have already shown what these patterns should be; now the task
 is to actually extract them from text.
\end_layout

\begin_layout Standard
The factorization is successful if 
\begin_inset Formula 
\[
\log_{2}\frac{p\left(w_{1},w_{2},\cdots,w_{n}\right)}{p\left(r_{1}\right)p\left(r_{2}\right)\cdots p\left(r_{k}\right)}\approx0
\]

\end_inset

With such a factorization in hand, one can now aim for higher and more abstract
 levels of structure, using the 
\begin_inset Formula $r_{i}$
\end_inset

 as the building blocks, rather than individual words.
 One should imagine a perturbative structure, each level givein a foundation
 for the next.
\end_layout

\begin_layout Subsection*
Example: the Binomial MI Formula
\end_layout

\begin_layout Standard
As a concrete example of the above, consider the mutual information 
\begin_inset Formula $MI\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 over 
\begin_inset Formula $n$
\end_inset

 variables.
 It is defined as
\begin_inset Formula 
\begin{equation}
MI\left(w_{1},w_{2},\cdots,w_{n}\right)=\sum_{k=0}^{n}\left(-1\right)^{n-k}\sum_{w\backslash k}\log_{2}p\left(\left\{ w\backslash k\right\} \right)\label{eq:binomial-MI}
\end{equation}

\end_inset

where 
\begin_inset Formula $\left\{ w\backslash k\right\} $
\end_inset

 is the set of words 
\begin_inset Formula $\left\{ w\right\} =\left\{ w_{1},w_{2},\cdots,w_{n}\right\} $
\end_inset

 with 
\begin_inset Formula $k$
\end_inset

 of them removed.
 The sum over 
\begin_inset Formula $w\backslash k$
\end_inset

 is a sum over every combinatoric possibility of removal.
 By 
\begin_inset Quotes eld
\end_inset

removed
\begin_inset Quotes erd
\end_inset

, it is meant 
\begin_inset Quotes eld
\end_inset

summed over
\begin_inset Quotes erd
\end_inset

, so that, for example, if 
\begin_inset Formula $w_{2}$
\end_inset

 is removed, then
\begin_inset Formula 
\[
p\left(\left\{ w\backslash w_{2}\right\} \right)=p\left(w_{1},*,w_{3},\cdots,w_{n}\right)=\sum_{w_{2}}p\left(w_{1},w_{2},w_{3},\cdots,w_{n}\right)
\]

\end_inset

The 
\begin_inset Formula $*$
\end_inset

 is the wild-card; it just denotes that 
\begin_inset Quotes eld
\end_inset

anything
\begin_inset Quotes erd
\end_inset

 can occupy that slot, and that, for probabilites, that slot should be summed
 over.
 Formally, 
\begin_inset Formula $\left\{ w\backslash w_{2}\right\} =\left(w_{1},*,w_{3},\cdots,w_{n}\right)$
\end_inset

 is called a 
\begin_inset Quotes eld
\end_inset

cylinder set
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Formula $p\left(\left\{ w\backslash w_{2}\right\} \right)$
\end_inset

 is a cylinder set measure.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia,
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset href
LatexCommand href
name "Cylinder set"
target "https://en.wikipedia.org/wiki/Cylinder_set"
literal "false"

\end_inset


\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 and
\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\xout off
\uuline off
\uwave off
\noun off
\color none
 
\begin_inset CommandInset href
LatexCommand href
name "Cylinder set measure"
target "https://en.wikipedia.org/wiki/Cylinder_set_measure"
literal "false"

\end_inset

.

\family default
\series default
\shape default
\size default
\emph default
\bar default
\strikeout default
\xout default
\uuline default
\uwave default
\noun default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The sum over 
\begin_inset Formula $w\backslash k$
\end_inset

 for 
\begin_inset Formula $k=1$
\end_inset

 is then a sum over all possible wildcard locations, for a single wildcard:
\begin_inset Formula 
\[
\sum_{w\backslash1}\log_{2}p\left(\left\{ w\backslash1\right\} \right)=\sum_{i=1}^{n}\log_{2}p\left(\left\{ w\backslash w_{i}\right\} \right)
\]

\end_inset

Likewise, for 
\begin_inset Formula $k=2$
\end_inset

 wildcards,
\begin_inset Formula 
\[
\sum_{w\backslash2}\log_{2}p\left(\left\{ w\backslash2\right\} \right)=\sum_{i=1}^{n}\sum_{j=1;j\ne i}^{n}\log_{2}p\left(\left\{ w\backslash\left\{ w_{i},w_{j}\right\} \right\} \right)
\]

\end_inset

and so on.
\end_layout

\begin_layout Standard
The alternating sign is such that the singletons 
\begin_inset Formula $p\left(w_{j}\right)=p\left(*,*,\cdots,w_{j},\cdots,*\right)$
\end_inset

 always have a minus sign in front of their log, while the first term is
 for the total space 
\begin_inset Formula $p\left(\left\{ w\backslash w\right\} \right)=p\left(\left\{ \varnothing\right\} \right)=p\left(*,*,\cdots,*\right)$
\end_inset

.
 If 
\begin_inset Formula $p\left(\left\{ \varnothing\right\} \right)=1$
\end_inset

 is a conventional probability, then of course 
\begin_inset Formula $\log_{2}p\left(\left\{ \varnothing\right\} \right)=0$
\end_inset

.
 However, this MI sum works just fine if 
\begin_inset Formula $p\left(\left\{ \varnothing\right\} \right)\ne1$
\end_inset

.
 For example, the binomial formula eqn 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:binomial-MI"
plural "false"
caps "false"
noprefix "false"

\end_inset

 still holds if 
\begin_inset Formula $N$
\end_inset

 a count is used instead of 
\begin_inset Formula $p$
\end_inset

.
 This is because the normalizing factor 
\begin_inset Formula $N\left(\left\{ \varnothing\right\} \right)$
\end_inset

 can be pulled back through all of the terms.
\end_layout

\begin_layout Standard
The size of the set 
\begin_inset Formula $\left\{ w\backslash k\right\} $
\end_inset

 is given by the binomial coefficient:
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Binomial coefficient"
target "https://en.wikipedia.org/wiki/Binomial_coefficient"
literal "false"

\end_inset


\end_layout

\end_inset


\begin_inset Formula 
\[
\left|\left\{ w\backslash k\right\} \right|={n \choose k}
\]

\end_inset

Note the resemblance of the formula for MI to the binomial theorem.
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See Wikipedia, 
\begin_inset CommandInset href
LatexCommand href
name "Binomial Theorem"
target "https://en.wikipedia.org/wiki/Binomial_theorem"
literal "false"

\end_inset


\end_layout

\end_inset

 This is not accidental; it is a generalization of the binomial formula
 that holds for non-uniform intervals and non-independent correlations.
 It reduces to exactly the binomial formula if all probabilities are independent
 and uniform in size,
\emph on
 i.e.

\emph default
 if 
\begin_inset Formula $p\left(a,b\right)=p\left(a\right)p\left(b\right)$
\end_inset

 and if 
\begin_inset Formula $p\left(w_{j}\right)=1/n$
\end_inset

.
 In this case, it becomes
\begin_inset Formula 
\begin{align*}
MI\left(w_{1},w_{2},\cdots,w_{n}\right)= & \sum_{k=0}^{n}\left(-1\right)^{n-k}{n \choose k}\log_{2}\frac{1}{n^{k}}\\
= & -\log_{2}n\cdot\sum_{k=0}^{n}\left(-1\right)^{n-k}k{n \choose k}\\
= & 0
\end{align*}

\end_inset

The last part follows from 
\begin_inset Formula 
\[
\frac{d}{dx}\left(1+x\right)^{n}=n\left(1+x\right)^{n-1}=\sum_{k=1}^{n}k{n \choose k}x^{k-1}
\]

\end_inset

and setting 
\begin_inset Formula $x=-1$
\end_inset

.
 
\end_layout

\begin_layout Standard
More generally, the binomial-MI formula follows from the Cartesian-product
 nature of the topology of sequences.
 It is a formula that holds generically for cylinder set measures; there
 is nothing language-specific in this example.
\end_layout

\begin_layout Standard
The point of this example is to show that something seemingly 
\begin_inset Quotes eld
\end_inset

large
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

complex
\begin_inset Quotes erd
\end_inset

, such as 
\begin_inset Formula $MI\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 can be reduced into smaller, perhaps more manageable components, which
 can then be recombined back into the whole, with an exact (not approximate)
 expression, a summation over pieces-parts.
\end_layout

\begin_layout Standard
But there is also a second lesson here: the binomial MI formula is not sutiable
 for natural language tasks.
 Although it is an exact expression, and individual words and word pairs
 appear in the lower summation terms, one gains no insight applying this
 to natural language.
 MI values can be both negative and positive; the alternating sign introduces
 more chaos into the mix.
 Basically, one has a series of small and large terms summing up and mostly
 cancelling one-another.
 (Literally: try this experimentally, if possible.
 You will find that the various MI's bounce around, getting large and small,
 and that the sum is almost always smaller than the largest term.) The quest
 is to find a similar expression, ideally, an exact expression, where most
 of the terms are strictly positive.
 This would allow the structure, the factorization to be approached perturbative
ly, as a strictly convergent sequence of corrections, each applied to the
 last.
\end_layout

\begin_layout Subsection*
Example: Syntactic Factorization
\end_layout

\begin_layout Standard
Parses imply factorizations.
 Consider a sentence with a fixed single parse.
 Suppose that there is a location 
\begin_inset Formula $i$
\end_inset

 in this parse, such that when considering the block of all words to the
 left of 
\begin_inset Formula $i$
\end_inset

, and the block of all words to the right of 
\begin_inset Formula $i$
\end_inset

, there is only a single edge connecting the two sides.
 For example, this might be the edge connecting word 
\begin_inset Formula $w_{i}$
\end_inset

, say, the verb, to word 
\begin_inset Formula $w_{j}$
\end_inset

, say, the object.
 (One cannot assume that 
\begin_inset Formula $j=i+1$
\end_inset

 since the object might have adjectives and determiners that precede it.)
 Then this parse implies a factorization 
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx p\left(w_{1},\cdots,w_{i}\right)p\left(r\left\{ w_{i},w_{j}\right\} \right)p\left(w_{i+1},\cdots,w_{n}\right)
\]

\end_inset

The is, the likelihood of the block of words to the left is effectively
 independent of the block of words on the right.
 
\end_layout

\begin_layout Standard
This factorization follows from the intuitive grammatical structure of natural
 language.
 Consider the sentence fragment 
\begin_inset Quotes eld
\end_inset

On alternate Tuesdays, John goes ...
\begin_inset Quotes erd
\end_inset

 How can it be completed? One imagines almost anything can complete it:
 
\begin_inset Quotes eld
\end_inset

...
 fishing in Georgetown.
\begin_inset Quotes erd
\end_inset

 
\begin_inset Quotes eld
\end_inset

...
 to the doctor.
\begin_inset Quotes erd
\end_inset

 That is, the completion of the sentence seems indpendent of the start of
 the sentence, and so the probability expression should factor like this
 as well.
\end_layout

\begin_layout Standard
Yet, this is just an approximation.
 Realizing that the first half of the sentence implies activity undertaken
 by a human, then the last half of the sentence must be an activity that
 humans can perform.
 So there is a linkage, a connection between these two parts of the sentence
 that extend beyond the grammatical relations between pairs of words.
 So again: the proposal here is to first factor according to syntax, providing
 a baseline, and then consider corrections to that initial factorization.
\end_layout

\begin_layout Standard
The above was written with a factor 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 specifically tying together the two specific words 
\begin_inset Formula $w_{i},w_{j}$
\end_inset

 connected by the parse edge.
 This factor is made explicit because one imagines that the specific word-choice
 connecting the left and right halves helps further isolate or make independent
 these two halves.
 In the example, it is presumed that 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 might capture at least some of the idea that the left side of the sentence
 is about humans, and the right side is about human activities.
\end_layout

\begin_layout Standard
Note that 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)\ne p\left(w_{i},w_{j}\right)$
\end_inset

 and that the probability depends on the relation 
\begin_inset Formula $r$
\end_inset

.
 That is, this factor is presumed to not be a simple word-pair co-occurance
 probability 
\begin_inset Formula $p\left(\mbox{"goes"},\mbox{"fishing"}\right)$
\end_inset

, but also includes a weighting for this word-pair being an auxiliary-verb
 pair.
 This now gives a first hint of the appearance of semantics in a syntactic
 discussion: The 
\begin_inset Formula $p\left(w_{i},w_{j}\right)$
\end_inset

 captures a syntactic relation between a pair of words; the 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 captures something more.
\end_layout

\begin_layout Subsubsection*
Connectors
\end_layout

\begin_layout Standard
Syntactic relations are not just pair-wise connections, though.
 Syntactic elements have more complex structure.
 Thus, the above factorization might be more correctly written as
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx\left[p\left(w_{1},\cdots,w_{i}\right)\sqrt{p\left(r\left\{ w_{i},w_{j}\right\} \right)}\right]\times\left[\sqrt{p\left(r\left\{ w_{i},w_{j}\right\} \right)}p\left(w_{i+1},\cdots,w_{n}\right)\right]
\]

\end_inset

so that half of 
\begin_inset Formula $p\left(r\left\{ w_{i},w_{j}\right\} \right)$
\end_inset

 rides along with word 
\begin_inset Formula $w_{i}$
\end_inset

 as the probability of making a connection to word 
\begin_inset Formula $w_{j}$
\end_inset

 and the other half rides with the other word.
 This introduces the concept of a connector probability: that it can be
 convenient to think in terms of connectors that have the potential to make
 a connection, rather than in terms of the actual connection.
\end_layout

\begin_layout Standard
The square-root term 
\begin_inset Formula $\sqrt{p\left(r\left\{ w_{i},w_{j}\right\} \right)}$
\end_inset

 will be refered to as the 
\begin_inset Quotes eld
\end_inset

connector probability
\begin_inset Quotes erd
\end_inset

.
 This concept is needed to recognize the fact that grammatical relations
 are not just pair-wise word relations, they are, in general, more complex.
 For example, a transitive verb 
\emph on
must
\emph default
 make a connection to both a subject on the left and an object on the right.
 This is effectively a triple 
\begin_inset Formula $\left(S,V,O\right)$
\end_inset

 and so, in principle, we should have instead factored as a tri-variable
 
\begin_inset Formula $p\left(\mbox{TrVb}\left\{ S,V,O\right\} \right)$
\end_inset

 instead of individual pair-wise word-links.
 The goal here is to recognize that a transitive verb 
\begin_inset Formula $V$
\end_inset

 has the potential to connect to a subject 
\begin_inset Formula $S$
\end_inset

, and the potential to connect to an object 
\begin_inset Formula $O$
\end_inset

.
 
\end_layout

\begin_layout Standard
This is kind of a sneaky way of saying 
\begin_inset Quotes eld
\end_inset

the grammar is actually Link Grammar
\begin_inset Quotes erd
\end_inset

, and the grammatical relation is 
\begin_inset Formula $\mbox{TrVb}\left\{ S,V,O\right\} =\mathtt{V:S-\,\&\,O+}$
\end_inset

.
 This is the conventional Link Grammar notation stating that the lexical
 entry 
\begin_inset Formula $\mathtt{V}$
\end_inset

 has the connector 
\begin_inset Formula $\mathtt{S-}$
\end_inset

 pointing to the left, and the connector 
\begin_inset Formula $\mathtt{O+}$
\end_inset

 pointing to the right.
 The combined expression 
\begin_inset Formula $\mathtt{S-\,\&\,O+}$
\end_inset

 is called a 
\begin_inset Quotes eld
\end_inset

disjunct
\begin_inset Quotes erd
\end_inset

; that is because it is disjoined from other lexical entries for the verb
 
\begin_inset Formula $\mathtt{V}$
\end_inset

.
\end_layout

\begin_layout Standard
The other sneaky thing done above was to introduce the idea of a 
\begin_inset Quotes eld
\end_inset

word class
\begin_inset Quotes erd
\end_inset

 (subjects, verbs, objects).
 The intended factorization, for transitive verbs, is to have written 
\begin_inset Formula 
\[
p
\]

\end_inset


\end_layout

\begin_layout Standard
This can be further strengthened by introducing the idea of a 
\begin_inset Quotes eld
\end_inset

word class
\begin_inset Quotes erd
\end_inset

, and writing the above as
\begin_inset Formula 
\begin{align*}
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx & \left[p\left(w_{1},\cdots,w_{i}\right)\sqrt{p\left(r\left\{ w_{i},V\right\} \right)}p\left(w_{j}\in V\right)\right]\times\\
 & \qquad\qquad\left[p\left(w_{i}\in A\right)\sqrt{p\left(r\left\{ A,w_{j}\right\} \right)}p\left(w_{i+1},\cdots,w_{n}\right)\right]
\end{align*}

\end_inset

where 
\begin_inset Formula $V$
\end_inset

 and 
\begin_inset Formula $A$
\end_inset

 are classes of words.
 For the earlier example, 
\begin_inset Formula $V$
\end_inset

 would be a collection of verbs that take auxilliaries, and 
\begin_inset Formula $A$
\end_inset

 would be the (small, almost closed set) of auxilliaries: have, be, may,
 do, shall, can, must.
\end_layout

\begin_layout Standard
The goal here is to allow factorization along the lines of conventional
 linguistic grammars, which work with word classes rather than single words.
 The first rough cut is provided by the conventional grammar, then an adjustment
 is made for the actual words employed.
\end_layout

\begin_layout Subsubsection*
Retracts
\end_layout

\begin_layout Subsubsection*
Twines
\end_layout

\begin_layout Section*
Factorization as Dimensional Embedding
\end_layout

\begin_layout Standard
Of course, syntax is not enough to convey the meaning in an expression,
 so the above approximations are necessarily mediocre.
 The factorization we are groping for should more properly be written as
 a change of variable
\begin_inset Formula 
\[
p\left(w_{1},w_{2},\cdots,w_{n}\right)\approx p\left(r_{1},r_{2},\cdots,r_{k}\right)
\]

\end_inset

where the variables 
\begin_inset Formula $r_{1},r_{2},\cdots,r_{k}$
\end_inset

 are in some sense 
\begin_inset Quotes eld
\end_inset

more independent
\begin_inset Quotes erd
\end_inset

 than the word-sequence 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

.
\end_layout

\begin_layout Standard
Note that in general, 
\begin_inset Formula $k\ne n$
\end_inset

.
 For spanning tree parses, the 
\begin_inset Formula $r_{i}$
\end_inset

 are understood to be links between word-pairs, and so 
\begin_inset Formula $k$
\end_inset

 is counting the number of links in the parse.
 Thus 
\begin_inset Formula $k=n-1$
\end_inset

 for word-pair relationships.
 If the parse has fundamental cycles (loops), then 
\begin_inset Formula $k=n-1+\ell$
\end_inset

 where 
\begin_inset Formula $\ell$
\end_inset

 is the number of fundamental cycles (
\emph on
e.g.

\emph default
 in an MPG parse).
\end_layout

\begin_layout Standard
Note that parsing performs a 
\begin_inset Quotes eld
\end_inset

dimensional oxidation
\begin_inset Quotes erd
\end_inset

: there are far more 
\begin_inset Formula $r_{i}$
\end_inset

's than there are 
\begin_inset Formula $w_{i}$
\end_inset

's.
 If the size of the base vocabulary is 
\begin_inset Formula $\mathcal{O}\left(\left|w\right|\right)\sim N$
\end_inset

 then 
\begin_inset Formula $\mathcal{O}\left(\left|r\right|\right)\sim N^{2}$
\end_inset

 where I'm using 
\begin_inset Formula $\mathcal{O}$
\end_inset

 notation because counting the size of the vocabulary is hard, when vocabulary
 words have a Zipfian distribution.
 Also, the claim that 
\begin_inset Formula $\mathcal{O}\left(\left|r\right|\right)\sim N^{2}$
\end_inset

 is somewhat misleading.
 Its actually more like 
\begin_inset Formula $\mathcal{O}\left(\left|r\right|\right)\sim N^{\gamma}$
\end_inset

 for some 
\begin_inset Formula $\gamma<2$
\end_inset

 because this is what the Zipfian distributions do to us.
 We've seen this experimentally, when we measure the sparsity and rarity,
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
See e.g.
 page 34 or Diary Part Five for rarity.
 I could have sworn I had this in othr tables, but I can't find it right
 now.
\end_layout

\end_inset

 but we haven't explicitly measured this.
\end_layout

\begin_layout Standard
Thus, MST parsing is a form of dimensional embedding, where the strings
 living in the relatively low-dimensional space 
\begin_inset Formula $\mathcal{O}\left(\left|\left(w_{1},w_{2},\cdots,w_{n}\right)\right|\right)\sim N^{n}$
\end_inset

 are embeded into the vastly larger space 
\begin_inset Formula $\mathcal{O}\left(\left|r_{1},r_{2},\cdots,r_{k}\right|\right)\sim N^{2k}$
\end_inset

.
\end_layout

\begin_layout Standard
In Link Grammar parsing, the embedding is not into a word-pair space, but
 into a disjunct space, which is explosively larger.
 That is, the relations 
\begin_inset Formula $r$
\end_inset

 are actually disjuncts 
\begin_inset Formula $d$
\end_inset

.
 I assume its 
\begin_inset Formula $\mathcal{O}\left(\left|d\right|\right)\sim N^{\gamma}$
\end_inset

 for some 
\begin_inset Formula $\gamma>2$
\end_inset

, but again, we've monitored this size without actually ever emasuring it's
 scaling dependence.
 Got to fix that.
\end_layout

\begin_layout Subsubsection*
Paths in hyperspace
\end_layout

\begin_layout Standard
Lets try to paint a mental image of this.
 Consider a vector space of dimension 
\begin_inset Formula $N$
\end_inset

, with 
\begin_inset Formula $N$
\end_inset

 the size of the vocabulary.
 Each 
\begin_inset Formula $w_{k}$
\end_inset

 is then a unit vector 
\begin_inset Formula $e_{k}$
\end_inset

 in this space.
 The word-sequence 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

 is a path in this space.
 The probabilities 
\begin_inset Formula $p\left(w_{1},w_{2},\cdots,w_{n}\right)$
\end_inset

 are hard to factorize, because there are many of these paths, and they
 overlap a lot.
\end_layout

\begin_layout Standard
Consider now a vector space of dimension 
\begin_inset Formula $D$
\end_inset

, with 
\begin_inset Formula $D$
\end_inset

 being the number of disjuncts.
 Very roughly, 
\begin_inset Formula $D\sim N^{\gamma}$
\end_inset

 for some 
\begin_inset Formula $\gamma>2$
\end_inset

 or somethig like that.
 So this is a much larger space.
 A single Link Grammar parse of a sentence 
\begin_inset Formula $w_{1},w_{2},\cdots,w_{n}$
\end_inset

 provides a unique sequence of disjuncts 
\begin_inset Formula $d_{1},d_{2},\cdots,d_{n}$
\end_inset

 fixed by that parse.
 As before, 
\begin_inset Formula $d_{1},d_{2},\cdots,d_{n}$
\end_inset

 specifies a path through the disjunct space.
 However, this time, the space is much larger, and so the accidental intersectio
n of two different paths is much less likely.
 There's disambiguation.
\end_layout

\begin_layout Subsubsection*
MST
\end_layout

\begin_layout Standard
The MST factorization is effectively the presumption that
\begin_inset Formula 
\[
MI\left(w_{1},w_{2},\cdots,w_{n}\right)\approx\sum_{\left(w_{i},w_{j}\right)\in MST}MI\left(w_{i},w_{j}\right)
\]

\end_inset

This sum runs only over the maximum spanning tree, and contains only 
\begin_inset Formula $n-1$
\end_inset

 links.
 The complete graph would have 
\begin_inset Formula $n\left(n-1\right)/2$
\end_inset

 links in it, and so most of these are ignored.
 Specifically, the presumption is that these ignored links actually cancel
 higher-order terms in the full MI expansion.
\end_layout

\begin_layout Standard
Put more plainly, it appears that
\begin_inset Formula 
\[
\log_{2}\frac{p\left(w_{1},w_{2},\cdots,w_{n}\right)}{p\left(w_{1}\right)p\left(w_{2}\right)\cdots p\left(w_{n}\right)}
\]

\end_inset

is, in general, 
\begin_inset Quotes eld
\end_inset

freakishly high
\begin_inset Quotes erd
\end_inset

, and that much of it can be 
\begin_inset Quotes eld
\end_inset

knocked down to size
\begin_inset Quotes erd
\end_inset

 by using the MST, instead.
 
\end_layout

\begin_layout Standard
What appears to be lacking is a coherent theoretical argument as to 
\emph on
why
\emph default
 an MST parse provides a reasonable approximation to the factorization.
 The next step is, of course, to use disjuncts, but again, without any clear
 argument about why the disjuncts provide a good approximation to the higher
 order terms in the sum of binomial-MI equation 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:binomial-MI"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
More precisely, the binomial-MI equation is correct, but unweildy, because
 it contains large cancelling terms.
 What is unclear is why these terms cancel, and how to best obtain a diagonalize
d, factorized perturbative expansion.
 The MST->disjunct path is just a gut-feel approach to obtaining that perturbati
ve expansion.
 It lacks formal justification for why it works.
\end_layout

\begin_layout Section*
The End
\end_layout

\begin_layout Standard
This is the end of Part Nine of the diary.
 
\end_layout

\end_body
\end_document
